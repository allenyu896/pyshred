{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4AZJexXQWg6"
      },
      "source": [
        "### SHRED applied to SST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujX2vv7PQWg6"
      },
      "source": [
        "This iPython notebook gives an introductory walkthrough to using SHRED models.  The dataset we consider is weekly mean sea-surface temperature as given by the NOAA Optimum Interpolation SST V2 dataset (https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.html).\n",
        "\n",
        "SHRED (SHallow REcurrent Decoder) models are a network architecture that merges a recurrent layer (LSTM) with a shallow decoder network (SDN) to reconstruct high-dimensional spatio-temporal fields from a trajectory of sensor measurements of the field. More formally, the SHRED architecture can be written as\n",
        "$$ \\mathcal {H} \\left( \\{ y_i \\} _{i=t-k}^t \\right) = \\mathcal {F} \\left( \\mathcal {G} \\left( \\{ y_i \\} _{i=t-k}^t \\right) ; W_{RN}) ; W_{SD} \\right)$$\n",
        "where $\\mathcal F$ is a feed forward network parameterized by weights $W_{SD}$, $\\mathcal G$ is a LSTM network parameterized by weights $W_{RN}$, and $\\{ y_i \\} _{i=t-k}^t$ is a trajectory of sensor measurements of a high-dimensional spatio-temporal field $\\{ x_i \\} _{i=t-k}^t$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zQuC7zJQWg7"
      },
      "source": [
        "We first randomly select 3 sensor locations and set the trajectory length (lags) to 52, corresponding to one year of measurements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AKdssdFnQWg7",
        "outputId": "a5bd0ab4-d60c-40f0-c3ad-17d30af792e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "could not read bytes",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-2004105203.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_sensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mload_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SST'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/processdata.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     22\u001b[0m     (N samples of m dimensional state)'''\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SST'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mload_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/SST_data.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mmean_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msst_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_X\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio5.py\u001b[0m in \u001b[0;36mget_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio5.py\u001b[0m in \u001b[0;36mread_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    289\u001b[0m            \u001b[0;31m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         '''\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.read_real_complex\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.read_numeric\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_mio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._mio5_utils.VarReader5.read_element\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_streams.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._streams.ZlibInputStream.read_string\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_streams.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab._streams.ZlibInputStream.read_into\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: could not read bytes"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from processdata import load_data\n",
        "from processdata import TimeSeriesDataset\n",
        "import models\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "num_sensors = 3\n",
        "lags = 52\n",
        "load_X = load_data('SST')\n",
        "n = load_X.shape[0]\n",
        "m = load_X.shape[1]\n",
        "sensor_locations = np.random.choice(m, size=num_sensors, replace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWaQcl67QWg7"
      },
      "source": [
        "We now select indices to divide the data into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a32t6O4QWg7"
      },
      "outputs": [],
      "source": [
        "train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
        "mask = np.ones(n - lags)\n",
        "mask[train_indices] = 0\n",
        "valid_test_indices = np.arange(0, n - lags)[np.where(mask!=0)[0]]\n",
        "valid_indices = valid_test_indices[::2]\n",
        "test_indices = valid_test_indices[1::2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seLQzrUMQWg7"
      },
      "source": [
        "sklearn's MinMaxScaler is used to preprocess the data for training and we generate input/output pairs for the training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyBJPPGeQWg7"
      },
      "outputs": [],
      "source": [
        "sc = MinMaxScaler()\n",
        "sc = sc.fit(load_X[train_indices])\n",
        "transformed_X = sc.transform(load_X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4zPxHeAQWg8"
      },
      "source": [
        "We now organize the data such that the inputs are of shape (batch_size, lags, num_sensors) with corresponding outputs of size (batch_size, state_dimension)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMNQKnrnQWg8"
      },
      "outputs": [],
      "source": [
        "### Generate input sequences to a SHRED model\n",
        "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
        "for i in range(len(all_data_in)):\n",
        "    all_data_in[i] = transformed_X[i:i+lags, sensor_locations]\n",
        "\n",
        "### Generate training validation and test datasets both for reconstruction of states and forecasting sensors\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
        "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
        "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
        "\n",
        "### -1 to have output be at the same time as final sensor measurements\n",
        "train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
        "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
        "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWYOSHpBQWg8"
      },
      "source": [
        "We train the model using the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zpSAiRBQWg8",
        "outputId": "bee88a44-41d2-4b4c-c6e4-50446d2750db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch 1\n",
            "Error tensor(0.4736)\n",
            "Training epoch 20\n",
            "Error tensor(0.2252)\n",
            "Training epoch 40\n",
            "Error tensor(0.2194)\n",
            "Training epoch 60\n",
            "Error tensor(0.2193)\n",
            "Training epoch 80\n",
            "Error tensor(0.2155)\n",
            "Training epoch 100\n",
            "Error tensor(0.2117)\n",
            "Training epoch 120\n",
            "Error tensor(0.2050)\n",
            "Training epoch 140\n",
            "Error tensor(0.2111)\n",
            "Training epoch 160\n",
            "Error tensor(0.2003)\n",
            "Training epoch 180\n",
            "Error tensor(0.1931)\n",
            "Training epoch 200\n",
            "Error tensor(0.1875)\n",
            "Training epoch 220\n",
            "Error tensor(0.1734)\n",
            "Training epoch 240\n",
            "Error tensor(0.1585)\n",
            "Training epoch 260\n",
            "Error tensor(0.1489)\n",
            "Training epoch 280\n",
            "Error tensor(0.1442)\n",
            "Training epoch 300\n",
            "Error tensor(0.1364)\n",
            "Training epoch 320\n",
            "Error tensor(0.1303)\n",
            "Training epoch 340\n",
            "Error tensor(0.1268)\n",
            "Training epoch 360\n",
            "Error tensor(0.1241)\n",
            "Training epoch 380\n",
            "Error tensor(0.1208)\n",
            "Training epoch 400\n",
            "Error tensor(0.1199)\n",
            "Training epoch 420\n",
            "Error tensor(0.1185)\n",
            "Training epoch 440\n",
            "Error tensor(0.1167)\n",
            "Training epoch 460\n",
            "Error tensor(0.1151)\n",
            "Training epoch 480\n",
            "Error tensor(0.1159)\n",
            "Training epoch 500\n",
            "Error tensor(0.1143)\n",
            "Training epoch 520\n",
            "Error tensor(0.1140)\n",
            "Training epoch 540\n",
            "Error tensor(0.1145)\n",
            "Training epoch 560\n",
            "Error tensor(0.1126)\n",
            "Training epoch 580\n",
            "Error tensor(0.1115)\n",
            "Training epoch 600\n",
            "Error tensor(0.1115)\n",
            "Training epoch 620\n",
            "Error tensor(0.1112)\n",
            "Training epoch 640\n",
            "Error tensor(0.1109)\n",
            "Training epoch 660\n",
            "Error tensor(0.1103)\n",
            "Training epoch 680\n",
            "Error tensor(0.1108)\n",
            "Training epoch 700\n",
            "Error tensor(0.1102)\n",
            "Training epoch 720\n",
            "Error tensor(0.1102)\n",
            "Training epoch 740\n",
            "Error tensor(0.1099)\n",
            "Training epoch 760\n",
            "Error tensor(0.1096)\n",
            "Training epoch 780\n",
            "Error tensor(0.1096)\n",
            "Training epoch 800\n",
            "Error tensor(0.1092)\n",
            "Training epoch 820\n",
            "Error tensor(0.1083)\n",
            "Training epoch 840\n",
            "Error tensor(0.1102)\n",
            "Training epoch 860\n",
            "Error tensor(0.1090)\n",
            "Training epoch 880\n",
            "Error tensor(0.1083)\n",
            "Training epoch 900\n",
            "Error tensor(0.1080)\n",
            "Training epoch 920\n",
            "Error tensor(0.1084)\n",
            "Training epoch 940\n",
            "Error tensor(0.1081)\n",
            "Training epoch 960\n",
            "Error tensor(0.1079)\n",
            "Training epoch 980\n",
            "Error tensor(0.1087)\n",
            "Training epoch 1000\n",
            "Error tensor(0.1074)\n"
          ]
        }
      ],
      "source": [
        "shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
        "validation_errors = models.fit(shred, train_dataset, valid_dataset, batch_size=64, num_epochs=1000, lr=1e-3, verbose=True, patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJGNeEniQWg8"
      },
      "source": [
        "Finally, we generate reconstructions from the test set and print mean square error compared to the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j10Pet5QWg8",
        "outputId": "c402adbe-416a-4ae1-e819-abc84f1e71b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Reconstruction Error: \n",
            "0.019373875\n"
          ]
        }
      ],
      "source": [
        "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
        "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
        "print('Test Reconstruction Error: ')\n",
        "print(np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "071f83251836d5bb3918d2af6501aef1a588d685a567aa45f470f25864dd9495"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}